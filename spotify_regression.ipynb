{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Kernel) Ridge Regression\n",
    "Download the Spotify Tracks Dataset and perform ridge regression to predict the tracksâ€™ popularity. Note that this dataset contains both numerical and categorical features. The student is thus required to follow these guidelines:\n",
    "- first, train the model using only the numerical features,\n",
    "- second, appropriately handle the categorical features (for example, with one-hot encoding or other techniques) and use them together with the numerical ones to train the model, in both cases, experiment with different training parameters, \n",
    "- use 5-fold cross validation to compute your risk estimates, thoroughly discuss and compare the performance of the model\n",
    "\n",
    "The student is required to implement from scratch (without using libraries, such as Scikit-learn) the code for the ridge regression, while it is not mandatory to do so for the implementation of the 5-fold cross-validation.\n",
    "\n",
    "Optional: Instead of regular ridge regression, implement kernel ridge regression using a Gaussian kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS\n",
    " - for each alpha, save the computed loss in order to draw a graph \n",
    " - try to avoid some features for seeing whether or not the loss decreases\n",
    " - compare our cv with the skitlearn cv\n",
    " - implementing a function that finds the predictor with the lowest loss in an alpha list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in str(get_ipython()):\n",
    "    !git clone https://github.com/lukebella/SpotifyRegression.git\n",
    "    !mv SpotifyRegression/* .\n",
    "    !rm -fr SpotifyRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
    "os.environ['KAGGLE_KEY'] = \"xxxxxx\"\n",
    "!kaggle datasets download -p ./data -d maharshipandya/-spotify-tracks-dataset\n",
    "!unzip -n ./data/-spotify-tracks-dataset.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset\n",
    "\n",
    "train_set = \"data/dataset.csv\"\n",
    "\n",
    "dataset_df = pd.read_csv(train_set).drop(columns='Unnamed: 0')\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training set and test set\n",
    "\n",
    "np.random.seed(0)\n",
    "mask = np.random.rand(len(dataset_df))<0.7\n",
    "\n",
    "train_df = dataset_df[mask]\n",
    "test_set = dataset_df[~mask]\n",
    "\n",
    "# y_train_df = train_df[[\"popularity\"]]\n",
    "# y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness_norm = dataset_df[\"loudness\"]\n",
    "tempo_norm = dataset_df[\"tempo\"]\n",
    "\n",
    "tempo_norm = tempo_norm/tempo_norm.max(axis=0)\n",
    "\n",
    "#(abs(min(col)) + i)/max+abs(min(col))\n",
    "\n",
    "min_loud = abs(loudness_norm.min(axis=0))\n",
    "max_loud = abs(loudness_norm.max(axis=0))\n",
    "\n",
    "loudness_norm = (min_loud+loudness_norm)/(min_loud+max_loud)\n",
    "dataset_df[\"loudness\"] = loudness_norm\n",
    "dataset_df[\"tempo\"] = tempo_norm\n",
    "\n",
    "#print(dataset_df[\"loudness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "numerical_df = dataset_df[[\"popularity\", \"duration_ms\", \"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]]\n",
    "\n",
    "train_num_df = numerical_df[mask]\n",
    "test_num_df = numerical_df[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "# One-hot encoding the categorical features using get_dummies\n",
    "categorical_df = pd.get_dummies(dataset_df.drop(columns= [\"track_id\", \"artists\", \"album_name\", \"track_name\"]), \n",
    "                                columns = ['explicit','key', 'mode', 'time_signature', 'track_genre'], dtype=int)\n",
    "\n",
    "train_cat_df = categorical_df[mask]\n",
    "test_cat_df = categorical_df[~mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hyperplane using regular ridge regression\n",
    "def ridge_regression(alpha, train_set):\n",
    "    y = train_set[[\"popularity\"]]\n",
    "    train_set = train_set.drop(columns='popularity')\n",
    "    n_rows, n_cols = train_set.shape  # Get the dimensions of the input matrix s\n",
    "    s_t = train_set.transpose()  # Transpose of matrix s\n",
    "    \n",
    "    # Calculate the identity matrix with the appropriate size\n",
    "    identity = np.identity(n_cols)\n",
    "    \n",
    "    # Calculate the ridge regression coefficients using matrix operations\n",
    "    w = (np.linalg.inv(alpha * identity + np.dot(s_t, train_set)).dot(s_t)).dot(y) \n",
    "    \n",
    "    # Convert the coefficients to a DataFrame for better presentation\n",
    "    w_df = pd.DataFrame(w, columns=[\"Values\"], index=train_set.columns)\n",
    "    \n",
    "    return w_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the popularity of a track x using an hyperplane w\n",
    "def predict(w, x):\n",
    "    pred = w.transpose().dot(x.drop(labels='popularity'))[0]\n",
    "    #pred = max(0, pred)\n",
    "    #return min(100, pred)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average square loss of the hyperplane w\n",
    "def avg_square_loss(w, test_set):\n",
    "    y = test_set[[\"popularity\"]]\n",
    "    test_set = test_set.drop(columns='popularity')\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    x = test_set.values  \n",
    "    # Calculate predictions for all rows at once\n",
    "    predictions = np.dot(x, w)\n",
    "    \n",
    "    squared_diff = (predictions -  y)**2\n",
    "    total_loss = np.sum(squared_diff)\n",
    "    return total_loss.values[0]/test_set.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression using only numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hyperplane for the numercal dataset\n",
    "result_numeric = ridge_regression(0.5, train_num_df)\n",
    "result_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the first row of the training set\n",
    "predicted_y = predict(result_numeric, train_num_df.iloc[4])\n",
    "print(f\"Predicted y: \\t{predicted_y}\\nReal y: \\t{train_num_df.iloc[4]['popularity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Average square loss of the hyperplane (numerical)\n",
    "print(\"Average square loss: \", avg_square_loss(result_numeric, test_num_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10, -2, 100)*0.5\n",
    "\n",
    "num_train_losses = []\n",
    "num_test_losses = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = ridge_regression(a, train_num_df)\n",
    "    num_train_losses.append(avg_square_loss(ridge, train_num_df))\n",
    "    num_test_losses.append(avg_square_loss(ridge, test_num_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('MSE numerical features')\n",
    "plt.plot(alphas, num_train_losses, label='Training accuracy')\n",
    "plt.plot(alphas, num_test_losses, label='Testing accuracy')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikitlearn Ridge regression on numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikitlearn Ridge regression\n",
    "\n",
    "alphas = 10**np.linspace(10, -2, 100)*0.5\n",
    "\n",
    "sk_num_train_losses = []\n",
    "sk_num_test_losses = []\n",
    "\n",
    "for a in alphas:\n",
    "    clf = Ridge(alpha = a)\n",
    "    clf.fit(train_num_df.drop(columns='popularity'), train_num_df['popularity'])\n",
    "    sk_num_train_losses.append(mean_squared_error(train_num_df['popularity'], clf.predict(train_num_df.drop(columns='popularity'))))\n",
    "    sk_num_test_losses.append(mean_squared_error(test_num_df['popularity'], clf.predict(test_num_df.drop(columns='popularity'))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('ScikitLearn MSE numerical features')\n",
    "plt.plot(alphas, sk_num_train_losses, label='Training accuracy')\n",
    "plt.plot(alphas, sk_num_test_losses, label='Testing accuracy')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression considering all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hyperplane for the numercal dataset\n",
    "result_categoric = ridge_regression(0.5, train_cat_df)\n",
    "result_categoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the first row of the training set\n",
    "predicted_y = predict(result_categoric, train_cat_df.iloc[0])\n",
    "print(f\"Predicted y: \\t{predicted_y}\\nReal y: \\t{train_cat_df.iloc[0]['popularity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Average square loss of the hyperplane (categorical)\n",
    "print(\"Average square loss: \", avg_square_loss(result_categoric, test_cat_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10, -2, 100)*0.5\n",
    "\n",
    "cat_train_losses = []\n",
    "cat_test_losses = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = ridge_regression(a, train_cat_df)\n",
    "    cat_train_losses.append(avg_square_loss(ridge, train_cat_df))\n",
    "    cat_test_losses.append(avg_square_loss(ridge, test_cat_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('MSE on all features')\n",
    "plt.plot(alphas, cat_train_losses, label='Training accuracy')\n",
    "plt.plot(alphas, cat_test_losses, label='Testing accuracy')\n",
    "# plt.plot(alphas, sk_cat_train_losses, label='SK Training accuracy')\n",
    "# plt.plot(alphas, sk_cat_test_losses, label='SK Testing accuracy')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScikitLearn Ridge regression on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikitlearn Ridge regression\n",
    "\n",
    "alphas = 10**np.linspace(10, -2, 100)*0.5\n",
    "\n",
    "sk_cat_train_losses = []\n",
    "sk_cat_test_losses = []\n",
    "\n",
    "for a in alphas:\n",
    "    clf = Ridge(alpha = a)\n",
    "    clf.fit(train_cat_df.drop(columns='popularity'), train_cat_df['popularity'])\n",
    "    sk_cat_train_losses.append(mean_squared_error(train_cat_df['popularity'], clf.predict(train_cat_df.drop(columns='popularity'))))\n",
    "    sk_cat_test_losses.append(mean_squared_error(test_cat_df['popularity'], clf.predict(test_cat_df.drop(columns='popularity'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('ScikitLearn MSE all features')\n",
    "plt.plot(alphas, sk_cat_train_losses, label='Training accuracy')\n",
    "plt.plot(alphas, sk_cat_test_losses, label='Testing accuracy')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical vs All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('MSE: numerical features vs. all features')\n",
    "plt.plot(alphas, num_train_losses, label='Num training accuracy')\n",
    "plt.plot(alphas, num_test_losses, label='Num testing accuracy')\n",
    "plt.plot(alphas, cat_train_losses, label='All training accuracy')\n",
    "plt.plot(alphas, cat_test_losses, label='All testing accuracy')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Nested) Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(k, dataset, alphas):\n",
    "\n",
    "    # Return a df from an arraty of df excepr the i-th\n",
    "    def get_set_except_i(dataset_array, i):\n",
    "        return pd.concat(dataset_array[j] for j in range(len(dataset_array)) if i!=j)\n",
    "    \n",
    "    # Split the dataset into k parts\n",
    "    dataset_array = np.array_split(dataset, k)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # In the i-th iteration, Si is the test and S\\Si is the training\n",
    "        test_cv = dataset_array[i] \n",
    "        train_cv = get_set_except_i(dataset_array, i)\n",
    "\n",
    "        # Split the training set into a new training set and a valid set (nested CV)\n",
    "        train_cv_array = np.array_split(train_cv, k-1)\n",
    "        dev_cv = train_cv_array[0]\n",
    "        nested_cv = get_set_except_i(train_cv_array, 0)\n",
    "        \n",
    "        # Find the best hyperparameter of your alphas\n",
    "        loss = float(\"inf\")\n",
    "        alpha = 0\n",
    "        for a in alphas:\n",
    "            predictor = ridge_regression(a, nested_cv)\n",
    "\n",
    "            local_loss = avg_square_loss(predictor, dev_cv)\n",
    "            if loss > local_loss:\n",
    "                loss = local_loss\n",
    "                alpha = a\n",
    "                \n",
    "        # Compute k predictors and their losses\n",
    "        prediction = ridge_regression(alpha, train_cv)\n",
    "        losses.append(avg_square_loss(prediction, test_cv))\n",
    "\n",
    "    #Find the avg loss of the predictors\n",
    "    return np.mean(losses), prediction, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "alphas = 10**np.linspace(10, -2, 100)*0.5\n",
    "loss_cv, pred_cv, a_cv = cross_validation(K, categorical_df[:1000], alphas)\n",
    "print(\"Average loss with nested CV: \", loss_cv)\n",
    "print(\"Best alpha with nested CV: \", a_cv)\n",
    "\n",
    "# Average loss with nested CV:  499.37945948919185\n",
    "# Best alpha with nested CV:  201.85086292982749"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScikitLearn Ridge CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10, -2, 100)*0.5\n",
    "\n",
    "clf = RidgeCV(alphas = alphas, cv= 5)\n",
    "clf.fit(categorical_df.drop(columns='popularity'), categorical_df['popularity'])\n",
    "\n",
    "mean_squared_error(categorical_df['popularity'], clf.predict(categorical_df.drop(columns='popularity')))\n",
    "\n",
    "#392.4801619277324\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(gamma, v1, v2):\n",
    "    norm = (np.linalg.norm(v1 - v2))**2\n",
    "    return [np.exp((norm)/-(2 * (gamma))), norm]   #(1/gamma*np.sqrt(2*np.pi)) *\n",
    "\n",
    "\n",
    "def kernel_ridge_regression(dataset, alpha, gamma = 0):\n",
    "    y = dataset[\"popularity\"]\n",
    "    dataset_values = dataset.drop(columns='popularity').values\n",
    "    n_samples = dataset.shape[0]\n",
    "    #print((np.linalg.norm(dataset_values[0]-dataset_values[1])**2)/2)\n",
    "    kernel = np.zeros((n_samples, n_samples))\n",
    "    if gamma == 0:\n",
    "        gamma = (np.linalg.norm(dataset_values[0]-dataset_values[1])**2)/2 + 1\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i,n_samples):\n",
    "                cell, gamma_new = gaussian_kernel(gamma, dataset_values[i], dataset_values[j])\n",
    "                kernel[i, j] = cell\n",
    "                gamma = (gamma + gamma_new)/2\n",
    "    else:\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i,n_samples):\n",
    "                cell, gamma_new = gaussian_kernel(gamma, dataset_values[i], dataset_values[j])\n",
    "                kernel[i, j] = cell\n",
    "    \n",
    "    #we consider half of the datapoints since it is the 'specular'\n",
    "    kernel = np.triu(kernel, 1) + kernel.transpose()  \n",
    "    #print(kernel)\n",
    "    identity = np.identity(n_samples)\n",
    "    #print(y.transpose())\n",
    "    #print(np.linalg.inv((alpha * identity + kernel)))\n",
    "    w = y.transpose() @ np.linalg.inv((alpha * identity + kernel))\n",
    "    #print(w)\n",
    "    w_df = pd.DataFrame(w, columns=['weights'])\n",
    "    return w_df\n",
    "\n",
    "\n",
    "def kernel_predict(w, dataset, x, gamma = 0):\n",
    "    x_values = x.drop(labels='popularity').values\n",
    "    dataset_values = dataset.drop(columns='popularity').values\n",
    "    #n_samples = dataset_values.shape[0]\n",
    "    kernel_values = []\n",
    "\n",
    "    if gamma == 0:\n",
    "        gamma = (np.linalg.norm(dataset_values[0]-x_values)**2)/2 +1\n",
    "\n",
    "        for x_i in dataset_values:\n",
    "            kernel_value, new_gamma = gaussian_kernel(gamma, x_values, x_i)\n",
    "            gamma = (new_gamma + gamma)/2\n",
    "            kernel_values.append(kernel_value)\n",
    "    else:\n",
    "        for x_i in dataset_values:\n",
    "            kernel_value, new_gamma = gaussian_kernel(gamma, x_values, x_i)\n",
    "            kernel_values.append(kernel_value)\n",
    "            \n",
    "    kernel_values = np.array(kernel_values)\n",
    "    prediction = w['weights'].dot(kernel_values)\n",
    "    prediction = max(0, prediction)  #ReLU\n",
    "    prediction = min(100, prediction)\n",
    "    return prediction\n",
    "\n",
    "def kernel_avg_square_loss(w, train_set, test_set, gamma=0):\n",
    "    y = test_set[[\"popularity\"]]\n",
    "    predictions = test_set.apply(lambda r: kernel_predict(w,train_set,r, gamma), 1) \n",
    "    squared_diff = (predictions - y.transpose())**2\n",
    "    return np.mean(squared_diff, axis=1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_cat_df[:1000]\n",
    "x = categorical_df.iloc[90]\n",
    "print(x['popularity'])\n",
    "w = kernel_ridge_regression(train_set, 1)\n",
    "print(kernel_predict(w, train_set, x))\n",
    "\n",
    "loss = kernel_avg_square_loss(w, train_set, test_cat_df[:5000])\n",
    "print(\"AVG Square loss: \", loss)\n",
    "print(\"AVG loss: \", loss**(1/2))\n",
    "# The predicted values must been adjusted, try to normalize all the values between 0 and 1 and multiply by 100 in the end for the popularity result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_cat_df[:1000]\n",
    "x = categorical_df.iloc[90]\n",
    "gamma = 10000000\n",
    "print(x['popularity'])\n",
    "w = kernel_ridge_regression(train_set, 1, gamma)\n",
    "print(kernel_predict(w, train_set, x, gamma))\n",
    "\n",
    "loss = kernel_avg_square_loss(w, train_set, test_cat_df[:1000], gamma)\n",
    "print(\"AVG Square loss: \", loss)\n",
    "print(\"AVG loss: \", loss**(1/2))\n",
    "# The predicted values must been adjusted, try to normalize all the values between 0 and 1 and multiply by 100 in the end for the popularity result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
